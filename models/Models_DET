#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug 30 10:06:13 2024

@author: djohan
"""



import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms


# Check if CUDA is available
if torch.cuda.is_available():
    # Set the default data type for tensors
    torch.set_default_dtype(torch.float32)
    # Set the default device to CUDA (GPU)
    torch.set_default_device('cuda')
    # Print a message indicating that CUDA is available

class Elephant(nn.Module):
    def __init__(self, a,d):
        super().__init__()
        self.a = a
        self.d = d
        
    def forward(self, x):
        return (1/(1+(x/self.a)**self.d)) 
    
class CNN_CIFAR_2_HEAD_DET(nn.Module):
    """
    Convolutional neural network used for task incremental learning on Split CIFAR10. 
    Same architecture as in F. Zenke "Continual Learning Through Synaptic Intelligence" (2017).
    """
    def __init__(self, ):
        super(CNN_CIFAR_2_HEAD_DET, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding='same')
        self.conv2 = nn.Conv2d(32, 32, 3)
        self.conv3 = nn.Conv2d(32, 64, 3, padding='same')
        self.conv4 = nn.Conv2d(64, 64, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.drop_conv = nn.Dropout(p=0.25)
        self.drop_fc = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(2304, 512)
        self.head1 = nn.Linear(512, 5)
        self.head2 = nn.Linear(512, 5)
        self.transform_train = transforms.RandomHorizontalFlip(0.5)
        #parameters for BBB 
        
    def forward(self, x, samples=1, head=1):
        """ 
        Forward pass through the network. 
        When samples=0, the model is deterministic, but we need samples_dim=1 for reshaping to work.
        """
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)

        x = F.relu(x)
        x = self.drop_conv(self.pool(x))

        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)

        x = F.relu(x)
        x = self.drop_conv(self.pool(x))
        

        print(x.shape)
        x = torch.flatten(x, 1)  # Flatten feature maps based on pooled dimensions
        print(x.shape)
        
        x = self.fc1(x)
        x = F.relu(x)
        x = self.drop_fc(x)
        
        
        
        # choose the head for split CIFAR, task incremental learning scenario with known task boundaries
        if head == 1:
            x = self.head1(x, samples)
        elif head == 2:
            x = self.head2(x, samples)
        
        x = F.log_softmax(x, dim=1)
        return x

    def loss(self, x, target, head=1):
        """
        Compute the Negative Log Likelihood
        """
        outputs = self.forward(self.transform_train(x), head=head)
        negative_log_likelihood = F.nll_loss(outputs, target, reduction='sum')
        return negative_log_likelihood 
    
    

    def conv_parameters(self, recurse: bool = True):
        """
        Yield parameters associated with convolutions.
        """
        for name, param in self.named_parameters(recurse=recurse):
            if name.startswith('conv'):
                yield param

    def fc_parameters(self, recurse: bool = True):
        """
        Yield parameters associated with fully connected layers.
        """
        for name, param in self.named_parameters(recurse=recurse):
            if name.startswith('fc'):
                yield param

   


class CNN_CIFAR_DET(nn.Module):
    """
    Convolutional neural network used for classic learning on CIFAR10. 
    Same architecture as in F. Zenke "Continual Learning Through Synaptic Intelligence" (2017).
    """
    def __init__(self, ):
        super(CNN_CIFAR_DET, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding='same')
        self.conv2 = nn.Conv2d(32, 32, 3)
        self.conv3 = nn.Conv2d(32, 64, 3, padding='same')
        self.conv4 = nn.Conv2d(64, 64, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.drop_conv = nn.Dropout(p=0.25)
        self.drop_fc = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(2304, 512)
        self.fc2 = nn.Linear(512, 10)
        self.transform_train = transforms.RandomHorizontalFlip(0.5)

    def forward(self, x, samples=1, head=1):
        """ 
        Forward pass through the network. 
        If in training mode we apply a random horizontal flip to the input
        """
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        
     
        x = F.relu(x)
        x = self.drop_conv(self.pool(x))
        
       
        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)
        
       
        x = F.relu(x)
        x = self.drop_conv(self.pool(x))
        
       
        x = torch.flatten(x, 2)  # Flatten feature maps based on pooled dimensions
        
        x = self.fc1(x)
        x = F.relu(x)
        x = self.drop_fc(x)
        x = self.fc2(x)
        
        x = F.log_softmax(x, dim=2)
        return x

    def loss(self, x, target, samples=1 ):
        """
        Compute the Negative Log Likelihood.
        """
        outputs = self.forward(self.transform_train(x), samples=samples)
        negative_log_likelihood = F.nll_loss(outputs.mean(0), target, reduction='sum')
        return negative_log_likelihood 
    
  
   
class FCNN_MNIST_DET(nn.Module):
    """
    Fully connected neural network used for classic and domain incremental learning on MNIST. 
    The activation function "elephant" is taken from:  Q. Lan "Elephant Neural Networks: Born to Be a Continual Learner" (2023).
    It provides better result for domain incremental learning. 
    This model is also used to measure the quality of the synaptic importance in EWC/SI/MESU. 
    """
    def __init__(self, num_in=784, num_out=10, hidden=256, a=1, d=2, activation='Tanh'):
        """
        Initialize the Bayesian Neural Network.
        """
        super(FCNN_MNIST_DET, self).__init__()
        
        self.fc1 = nn.Linear(num_in, hidden)
        self.fc2 = nn.Linear(hidden, num_out)
        #parameters for the elephant activation function
        self.a = a
        self.d = d

        valid_activations = {'Tanh', 'Relu', 'Elephant'}
        if activation not in valid_activations:
            raise ValueError(
                "Invalid activation name {!r}, should be one of {}".format(
                    activation, valid_activations))    
            
        if activation=='Tanh':
            self.act=torch.nn.Tanh()
            
        if activation=='Relu':
            self.act=torch.nn.ReLU()
            
        if activation=='Elephant':
            self.act=Elephant(a=self.a,d=self.d) 
   
    def forward(self, x, samples=1):
        """
        Forward propagation through the network.
        """
        x = self.fc1(x, samples)
        x = self.act(x)
        x = self.fc2(x, samples)
        x = F.log_softmax(x, dim=2)
        return x
    
    def loss(self, x, target, samples=1):
        """
        Compute the Negative Log Likelihood.
        """
        outputs = self.forward(x, samples=samples)
        negative_log_likelihood = F.nll_loss(outputs.mean(0), target, reduction='sum')
        return negative_log_likelihood 
    
   
    # Below are some custom functions that can be used for specific experiments
    def calculate_importance(self,):
        """
        For Synaptic intelligence
        function taken from https://github.com/GT-RIPL/Continual-Learning-Benchmark/blob/master/agents/regularization.py
        """

        importance = {}
        for idx, p in enumerate(self.parameters()):
            importance[idx] = p.clone().detach().fill_(0)  # zero initialized
        prev_params = self.initial_params
    
        # Calculate or accumulate the Omega (the importance matrix)
        for idx, p in importance.items():
            delta_theta = list(self.parameters())[idx].detach() - prev_params[idx]
            p += self.w[idx] / (delta_theta**2 + self.damping_factor)
            self.w[idx].zero_()
    
        return importance
    
